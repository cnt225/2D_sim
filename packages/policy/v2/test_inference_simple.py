#!/usr/bin/env python3
import sys
import torch
import numpy as np
from pathlib import Path

sys.path.append('/home/dhkang225/2D_sim/packages/policy/v2')

from models import get_model
from loaders import get_dataloader
from omegaconf import OmegaConf

def test_simple_inference():
    """ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Simple Inference Test...")
    
    # Config ë¡œë“œ
    cfg = OmegaConf.load('configs/motion_rcfm.yml')
    
    # ëª¨ë¸ ìƒì„±
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = get_model(cfg['model']).to(device)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì†ë„ ê¸°ë°˜ ëª¨ë¸)
    checkpoint_path = 'train_results/motion_rcfm/20250820-0025/model_best_normalized.pth'
    checkpoint = torch.load(checkpoint_path, weights_only=False)
    
    # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸
    print(f"ğŸ“¦ Loading checkpoint: {checkpoint_path}")
    print(f"ğŸ”§ Checkpoint keys: {list(checkpoint.keys())}")
    
    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    elif 'model_state' in checkpoint:
        model.load_state_dict(checkpoint['model_state'])
    else:
        # Direct state dict
        model.load_state_dict(checkpoint)
    
    model.eval()
    print("âœ… Model loaded successfully!")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_cfg = cfg['data']['test']
    dataloader = get_dataloader(test_cfg)
    batch = next(iter(dataloader))
    
    # GPUë¡œ ì´ë™
    for k, v in batch.items():
        if isinstance(v, torch.Tensor):
            batch[k] = v.to(device)
    
    print(f"ğŸ“Š Test batch loaded: {batch['current_T'].shape[0]} samples")
    
    # ì¶”ë¡  ì‹¤í–‰
    with torch.no_grad():
        # í¬ì¸íŠ¸í´ë¼ìš°ë“œ íŠ¹ì§• ì¶”ì¶œ
        pc_features = model.get_latent_vector(batch['pc'])
        
        # T_dot ì˜ˆì¸¡
        T_dot_pred = model.forward(
            batch['current_T'], 
            batch['target_T'], 
            batch['time_t'], 
            pc_features, 
            batch['g']
        )
        
        print(f"ğŸ¯ Prediction successful!")
        print(f"   Input shapes:")
        print(f"     current_T: {batch['current_T'].shape}")
        print(f"     target_T: {batch['target_T'].shape}")
        print(f"     time_t: {batch['time_t'].shape}")
        print(f"     pc: {batch['pc'].shape}")
        print(f"     g: {batch['g'].shape}")
        print(f"   Output shape: {T_dot_pred.shape}")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„
        T_dot_gt = batch['T_dot']
        mse_loss = torch.nn.functional.mse_loss(T_dot_pred, T_dot_gt)
        
        print(f"ğŸ“ˆ Results:")
        print(f"   MSE Loss: {mse_loss.item():.6f}")
        print(f"   Predicted T_dot range: [{T_dot_pred.min().item():.4f}, {T_dot_pred.max().item():.4f}]")
        print(f"   Ground truth T_dot range: [{T_dot_gt.min().item():.4f}, {T_dot_gt.max().item():.4f}]")
        
        # ìƒ˜í”Œë³„ ë¹„êµ
        for i in range(min(2, T_dot_pred.shape[0])):
            print(f"\n   Sample {i}:")
            print(f"     Predicted: {T_dot_pred[i].cpu().numpy()}")
            print(f"     Actual:    {T_dot_gt[i].cpu().numpy()}")
            print(f"     Error:     {torch.abs(T_dot_pred[i] - T_dot_gt[i]).cpu().numpy()}")

if __name__ == "__main__":
    test_simple_inference()
