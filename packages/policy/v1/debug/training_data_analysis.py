#!/usr/bin/env python3
"""
í•™ìŠµ ë°ì´í„°ì˜ twist vector í¬ê¸°ì™€ ë¶„í¬ ë¶„ì„
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import json
import glob
import os
import sys
from pathlib import Path

sys.path.append('.')
from loaders.trajectory_dataset import TrajectoryDataset

def analyze_training_data():
    """í•™ìŠµ ë°ì´í„°ì˜ twist vector ë¶„ì„"""
    print("ğŸ” í•™ìŠµ ë°ì´í„° ë¶„ì„")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    trajectory_root = "../../../data/trajectories/circle_envs_10k_bsplined"
    pointcloud_root = "../../../data/pointcloud/circle_envs_10k/circle_envs_10k"
    
    print(f"ğŸ“‚ ê¶¤ì  ë°ì´í„°: {trajectory_root}")
    print(f"ğŸ“‚ í¬ì¸íŠ¸í´ë¼ìš°ë“œ: {pointcloud_root}")
    
    # ì‘ì€ ì„œë¸Œì…‹ìœ¼ë¡œ ë¹ ë¥¸ ë¶„ì„
    dataset = TrajectoryDataset(
        trajectory_root=trajectory_root,
        pointcloud_root=pointcloud_root,
        split='train',
        max_trajectories=50,  # ë¹ ë¥¸ ë¶„ì„ì„ ìœ„í•´ 50ê°œë§Œ
        use_bsplined=True,
        augment_data=False,
        num_points=300
    )
    
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ")
    print()
    
    # Twist vector í†µê³„ ìˆ˜ì§‘
    twist_norms = []
    linear_norms = []
    angular_norms = []
    timestamps = []
    waypoint_distances = []
    
    print("ğŸ“Š ìƒ˜í”Œ ë¶„ì„ ì¤‘...")
    
    for i in range(min(len(dataset), 1000)):  # 1000ê°œ ìƒ˜í”Œë§Œ ë¶„ì„
        if i % 100 == 0:
            print(f"   ì§„í–‰ë¥ : {i}/{min(len(dataset), 1000)}")
        
        try:
            sample = dataset[i]
            T_dot = sample['T_dot'].numpy()
            
            # Angular part (first 3)
            angular = T_dot[:3]
            angular_norm = np.linalg.norm(angular)
            
            # Linear part (last 3)
            linear = T_dot[3:]
            linear_norm = np.linalg.norm(linear)
            
            # Total norm
            total_norm = np.linalg.norm(T_dot)
            
            twist_norms.append(total_norm)
            linear_norms.append(linear_norm)
            angular_norms.append(angular_norm)
            
            # ì‹œê°„ ì •ë³´
            time_t = sample['time_t'].item()
            timestamps.append(time_t)
            
            # í˜„ì¬-ëª©í‘œ ê±°ë¦¬
            current_T = sample['current_T'].numpy()
            target_T = sample['target_T'].numpy()
            distance = np.linalg.norm(target_T[:3, 3] - current_T[:3, 3])
            waypoint_distances.append(distance)
            
        except Exception as e:
            continue
    
    # í†µê³„ ê³„ì‚°
    twist_norms = np.array(twist_norms)
    linear_norms = np.array(linear_norms)
    angular_norms = np.array(angular_norms)
    timestamps = np.array(timestamps)
    waypoint_distances = np.array(waypoint_distances)
    
    print(f"\nğŸ“ˆ Twist Vector í†µê³„ (ì´ {len(twist_norms)}ê°œ ìƒ˜í”Œ):")
    print(f"   ğŸ”„ Angular velocity:")
    print(f"      í‰ê· : {np.mean(angular_norms):.6f} rad/s")
    print(f"      í‘œì¤€í¸ì°¨: {np.std(angular_norms):.6f}")
    print(f"      ìµœëŒ€: {np.max(angular_norms):.6f}")
    print(f"      ìµœì†Œ: {np.min(angular_norms):.6f}")
    print()
    
    print(f"   ğŸ“ Linear velocity:")
    print(f"      í‰ê· : {np.mean(linear_norms):.6f} m/s")
    print(f"      í‘œì¤€í¸ì°¨: {np.std(linear_norms):.6f}")
    print(f"      ìµœëŒ€: {np.max(linear_norms):.6f}")
    print(f"      ìµœì†Œ: {np.min(linear_norms):.6f}")
    print()
    
    print(f"   ğŸ“Š Total twist:")
    print(f"      í‰ê· : {np.mean(twist_norms):.6f}")
    print(f"      í‘œì¤€í¸ì°¨: {np.std(twist_norms):.6f}")
    print(f"      ìµœëŒ€: {np.max(twist_norms):.6f}")
    print(f"      ìµœì†Œ: {np.min(twist_norms):.6f}")
    print()
    
    print(f"   ğŸ“ ê±°ë¦¬ ì •ë³´:")
    print(f"      í‰ê·  í˜„ì¬-ëª©í‘œ ê±°ë¦¬: {np.mean(waypoint_distances):.6f} m")
    print(f"      í‘œì¤€í¸ì°¨: {np.std(waypoint_distances):.6f}")
    print(f"      ìµœëŒ€: {np.max(waypoint_distances):.6f}")
    print(f"      ìµœì†Œ: {np.min(waypoint_distances):.6f}")
    
    return {
        'twist_norms': twist_norms,
        'linear_norms': linear_norms,
        'angular_norms': angular_norms,
        'timestamps': timestamps,
        'waypoint_distances': waypoint_distances
    }

def analyze_single_trajectory():
    """ë‹¨ì¼ ê¶¤ì  ìƒì„¸ ë¶„ì„"""
    print("\nğŸ” ë‹¨ì¼ ê¶¤ì  ìƒì„¸ ë¶„ì„")
    print("=" * 50)
    
    # í•˜ë‚˜ì˜ ê¶¤ì  íŒŒì¼ ì§ì ‘ ë¡œë“œ  
    traj_file = "../../../data/trajectories/circle_envs_10k_bsplined/circle_env_000000_pair_1_traj_rb3_bsplined.json"
    
    if not os.path.exists(traj_file):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {traj_file}")
        return
    
    with open(traj_file, 'r') as f:
        traj_data = json.load(f)
    
    path_data = traj_data.get('path', {})
    poses_flat = path_data.get('data', [])
    timestamps = path_data.get('timestamps', [])
    
    print(f"ğŸ“„ íŒŒì¼: {os.path.basename(traj_file)}")
    print(f"ğŸ“Š ì›¨ì´í¬ì¸íŠ¸ ìˆ˜: {len(poses_flat)}")
    print(f"â±ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜: {len(timestamps)}")
    
    # ê° ì›¨ì´í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ì™€ ì‹œê°„ ê°„ê²© ë¶„ì„
    if len(poses_flat) < 2:
        print("âŒ ì›¨ì´í¬ì¸íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
        return
    
    step_distances = []
    time_intervals = []
    computed_velocities = []
    
    for i in range(len(poses_flat) - 1):
        # í˜„ì¬ì™€ ë‹¤ìŒ í¬ì¦ˆ
        pose1 = poses_flat[i][:3]  # [x, y, z]
        pose2 = poses_flat[i + 1][:3]
        
        # ê±°ë¦¬
        distance = np.linalg.norm(np.array(pose2) - np.array(pose1))
        step_distances.append(distance)
        
        # ì‹œê°„ ê°„ê²©
        if i < len(timestamps) - 1:
            dt = timestamps[i + 1] - timestamps[i]
        else:
            dt = 0.1  # ê¸°ë³¸ê°’
        time_intervals.append(dt)
        
        # ì†ë„
        if dt > 0:
            velocity = distance / dt
            computed_velocities.append(velocity)
    
    step_distances = np.array(step_distances)
    time_intervals = np.array(time_intervals)
    computed_velocities = np.array(computed_velocities)
    
    print(f"\nğŸ“ ì›¨ì´í¬ì¸íŠ¸ ê°„ ê±°ë¦¬:")
    print(f"   í‰ê· : {np.mean(step_distances):.6f} m")
    print(f"   í‘œì¤€í¸ì°¨: {np.std(step_distances):.6f}")
    print(f"   ìµœëŒ€: {np.max(step_distances):.6f}")
    print(f"   ìµœì†Œ: {np.min(step_distances):.6f}")
    
    print(f"\nâ±ï¸ ì‹œê°„ ê°„ê²©:")
    print(f"   í‰ê· : {np.mean(time_intervals):.6f} s")
    print(f"   í‘œì¤€í¸ì°¨: {np.std(time_intervals):.6f}")
    print(f"   ìµœëŒ€: {np.max(time_intervals):.6f}")
    print(f"   ìµœì†Œ: {np.min(time_intervals):.6f}")
    
    print(f"\nğŸš€ ê³„ì‚°ëœ ì†ë„:")
    print(f"   í‰ê· : {np.mean(computed_velocities):.6f} m/s")
    print(f"   í‘œì¤€í¸ì°¨: {np.std(computed_velocities):.6f}")
    print(f"   ìµœëŒ€: {np.max(computed_velocities):.6f}")
    print(f"   ìµœì†Œ: {np.min(computed_velocities):.6f}")
    
    return {
        'step_distances': step_distances,
        'time_intervals': time_intervals,
        'computed_velocities': computed_velocities
    }

def check_normalization_in_training():
    """í•™ìŠµ ì½”ë“œì—ì„œ ì •ê·œí™” ì—¬ë¶€ í™•ì¸"""
    print("\nğŸ” í•™ìŠµ ê³¼ì • ì •ê·œí™” í™•ì¸")
    print("=" * 50)
    
    # Train ìŠ¤í¬ë¦½íŠ¸ í™•ì¸
    train_files = glob.glob("train*.py")
    
    for train_file in train_files:
        print(f"ğŸ“„ {train_file} í™•ì¸ ì¤‘...")
        
        with open(train_file, 'r') as f:
            content = f.read()
        
        # ì •ê·œí™” ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
        keywords = ['normalize', 'norm', 'scale', 'std', 'mean']
        found_lines = []
        
        lines = content.split('\n')
        for i, line in enumerate(lines):
            for keyword in keywords:
                if keyword.lower() in line.lower() and 'T_dot' in line:
                    found_lines.append(f"   Line {i+1}: {line.strip()}")
        
        if found_lines:
            print(f"   ğŸ” ì •ê·œí™” ê´€ë ¨ ì½”ë“œ ë°œê²¬:")
            for line in found_lines[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(line)
        else:
            print(f"   âŒ ì •ê·œí™” ê´€ë ¨ ì½”ë“œ ì—†ìŒ")
        print()

def visualize_data_distribution(data):
    """ë°ì´í„° ë¶„í¬ ì‹œê°í™”"""
    print("\nğŸ“Š ë°ì´í„° ë¶„í¬ ì‹œê°í™”")
    print("=" * 50)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. Linear velocity ë¶„í¬
    axes[0, 0].hist(data['linear_norms'], bins=50, alpha=0.7)
    axes[0, 0].set_title('Linear Velocity Distribution')
    axes[0, 0].set_xlabel('Speed (m/s)')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].axvline(np.mean(data['linear_norms']), color='red', linestyle='--', 
                       label=f'Mean: {np.mean(data["linear_norms"]):.4f}')
    axes[0, 0].legend()
    
    # 2. Angular velocity ë¶„í¬
    axes[0, 1].hist(data['angular_norms'], bins=50, alpha=0.7)
    axes[0, 1].set_title('Angular Velocity Distribution')
    axes[0, 1].set_xlabel('Angular Speed (rad/s)')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].axvline(np.mean(data['angular_norms']), color='red', linestyle='--',
                       label=f'Mean: {np.mean(data["angular_norms"]):.4f}')
    axes[0, 1].legend()
    
    # 3. ê±°ë¦¬ vs Linear velocity
    axes[1, 0].scatter(data['waypoint_distances'], data['linear_norms'], alpha=0.5)
    axes[1, 0].set_xlabel('Distance to Target (m)')
    axes[1, 0].set_ylabel('Linear Velocity (m/s)')
    axes[1, 0].set_title('Distance vs Linear Velocity')
    
    # 4. Time vs Linear velocity
    axes[1, 1].scatter(data['timestamps'], data['linear_norms'], alpha=0.5)
    axes[1, 1].set_xlabel('Normalized Time')
    axes[1, 1].set_ylabel('Linear Velocity (m/s)')
    axes[1, 1].set_title('Time vs Linear Velocity')
    
    plt.tight_layout()
    plt.savefig('training_data_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… ì‹œê°í™” ì €ì¥: training_data_analysis.png")

if __name__ == "__main__":
    print("ğŸš€ í•™ìŠµ ë°ì´í„° ë¶„ì„ ì‹œì‘")
    print()
    
    # 1. í•™ìŠµ ë°ì´í„° ì „ì²´ ë¶„ì„
    data = analyze_training_data()
    
    # 2. ë‹¨ì¼ ê¶¤ì  ìƒì„¸ ë¶„ì„
    single_traj_data = analyze_single_trajectory()
    
    # 3. í•™ìŠµ ì½”ë“œ ì •ê·œí™” í™•ì¸
    check_normalization_in_training()
    
    # 4. ì‹œê°í™”
    if data and len(data['twist_norms']) > 0:
        visualize_data_distribution(data)
    
    print("\n" + "="*60)
    print("ğŸ¯ í•™ìŠµ ë°ì´í„° ë¶„ì„ ê²°ê³¼")
    print("="*60)
    
    if data:
        avg_linear = np.mean(data['linear_norms'])
        avg_angular = np.mean(data['angular_norms'])
        
        print(f"ğŸ“Š í•µì‹¬ ë°œê²¬:")
        print(f"   1. í•™ìŠµ ë°ì´í„° í‰ê·  ì„ ì†ë„: {avg_linear:.6f} m/s")
        print(f"   2. ëª¨ë¸ ì˜ˆì¸¡ í‰ê·  ì„ ì†ë„: ~0.06 m/s")
        print(f"   3. ì°¨ì´ ë¹„ìœ¨: {0.06/avg_linear:.2f}ë°°")
        print()
        
        if avg_linear > 0.1:
            print("âœ… í•™ìŠµ ë°ì´í„°ëŠ” ì¶©ë¶„íˆ í° ì†ë„ë¥¼ ê°€ì§€ê³  ìˆìŒ")
            print("âŒ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë³´ë‹¤ í›¨ì”¬ ì‘ì€ ì†ë„ ì˜ˆì¸¡")
            print("ğŸ”§ ê°€ëŠ¥í•œ ì›ì¸:")
            print("   - ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ê³¼ë„í•œ ì •ê·œí™”")
            print("   - í•™ìŠµë¥ ì´ë‚˜ ë°°ì¹˜ í¬ê¸° ë¬¸ì œ") 
            print("   - ëª¨ë¸ ìš©ëŸ‰ ë¶€ì¡±")
        else:
            print("âŒ í•™ìŠµ ë°ì´í„° ìì²´ê°€ ì‘ì€ ì†ë„ë¥¼ ê°€ì§")
            print("ğŸ”§ ë°ì´í„° ìŠ¤ì¼€ì¼ ì¡°ì • í•„ìš”")
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
