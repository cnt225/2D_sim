#!/usr/bin/env python3
"""
SE(3) Trajectory Dataset for PyTorch
HDF5 ê¸°ë°˜ SE(3) ê¶¤ì  ë°ì´í„°ë¥¼ ìœ„í•œ PyTorch Dataset í´ë˜ìŠ¤
fm-main íŒ¨í„´ì„ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„
"""

import torch
import torch.utils.data
import numpy as np
import h5py
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
from copy import deepcopy
import sys

# Import SE(3) functions and HDF5 loader
sys.path.append('/Users/a123/Documents/Projects/2D_sim/packages/utils')
sys.path.append('/Users/a123/Documents/Projects/2D_sim/packages/data_generator/hdf5_tools')

from SE3_functions import (
    traj_smooth_se3_bspline_slerp,
    traj_process_se3_pipeline,
    traj_build_labels_with_policy,
    traj_integrate_by_twist,
    _se3_exp,
    _se3_log,
    trajectory_quaternion_to_euler,
    euler_6d_to_quaternion_7d
)
from hdf5_trajectory_loader import HDF5TrajectoryLoader


class SE3TrajectoryDataset(torch.utils.data.Dataset):
    """
    SE(3) ê¶¤ì  ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (fm-main íŒ¨í„´ ì°¸ì¡°)
    
    Features:
    - HDF5 ê¸°ë°˜ ë°ì´í„° ë¡œë”©
    - SE(3) ê¶¤ì  ìŠ¤ë¬´ë”© ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    - Arc-length ë¦¬ìƒ˜í”Œë§ ë° ì‹œê°„ ì •ì±…
    - Body twist ë¼ë²¨ ìƒì„±
    - ë°ì´í„° ì¦ê°• (íšŒì „, ë…¸ì´ì¦ˆ ë“±)
    - ë°°ì¹˜ ì²˜ë¦¬ ë° í…ì„œ ë³€í™˜
    """
    
    def __init__(self, 
                 hdf5_path: str,
                 split: str = 'train',
                 env_ids: Optional[List[str]] = None,
                 rigid_body_ids: Optional[List[int]] = None,
                 trajectory_type: str = 'raw',
                 
                 # SE(3) ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
                 use_smoothing: bool = True,
                 smooth_strength: float = 0.1,
                 num_samples: int = 200,
                 lambda_rot: float = 0.0,
                 time_policy: str = "curvature",
                 v_ref: float = 0.4,
                 v_cap: float = 0.5,
                 a_lat_max: float = 1.0,
                 
                 # ë°ì´í„° ì¦ê°•
                 augmentation: bool = True,
                 rotation_noise_std: float = 0.1,
                 position_noise_std: float = 0.05,
                 
                 # ë°°ì¹˜ ì„¤ì •
                 max_trajectories: Optional[int] = None,
                 **kwargs):
        """
        Args:
            hdf5_path: HDF5 ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            split: ë°ì´í„° ë¶„í•  ('train', 'valid', 'test')
            env_ids: ì‚¬ìš©í•  í™˜ê²½ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  í™˜ê²½)
            rigid_body_ids: ì‚¬ìš©í•  ë¡œë´‡ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ë¡œë´‡)
            trajectory_type: ê¶¤ì  íƒ€ì… ('raw' | 'bsplined')
            
            # SE(3) ì²˜ë¦¬
            use_smoothing: SE(3) ìŠ¤ë¬´ë”© ì‚¬ìš© ì—¬ë¶€
            smooth_strength: ìŠ¤ë¬´ë”© ê°•ë„ (0.0=ë³´ê°„, >0=ìŠ¤ë¬´ë”©)
            num_samples: ë¦¬ìƒ˜í”Œë§ í¬ì¸íŠ¸ ìˆ˜
            lambda_rot: íšŒì „ ê°€ì¤‘ì¹˜ (arc-length ê³„ì‚° ì‹œ)
            time_policy: ì‹œê°„ ì •ì±… ("uniform" | "curvature")
            v_ref: ê¸°ì¤€ ì†ë„
            v_cap: ìµœëŒ€ ì†ë„
            a_lat_max: ìµœëŒ€ íš¡ê°€ì†ë„
            
            # ì¦ê°•
            augmentation: ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€
            rotation_noise_std: íšŒì „ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
            position_noise_std: ìœ„ì¹˜ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
            
            # ê¸°íƒ€
            max_trajectories: ìµœëŒ€ ê¶¤ì  ìˆ˜ ì œí•œ
        """
        self.hdf5_path = Path(hdf5_path)
        self.split = split
        self.trajectory_type = trajectory_type
        
        # SE(3) ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
        self.use_smoothing = use_smoothing
        self.smooth_strength = smooth_strength
        self.num_samples = num_samples
        self.lambda_rot = lambda_rot
        self.time_policy = time_policy
        self.v_ref = v_ref
        self.v_cap = v_cap
        self.a_lat_max = a_lat_max
        
        # ì¦ê°• íŒŒë¼ë¯¸í„°
        self.augmentation = augmentation and (split == 'train')
        self.rotation_noise_std = rotation_noise_std
        self.position_noise_std = position_noise_std
        
        # HDF5 ë¡œë” ì´ˆê¸°í™”
        self.hdf5_loader = HDF5TrajectoryLoader(str(self.hdf5_path))
        
        # ë°ì´í„° ìˆ˜ì§‘
        self._collect_trajectory_data(env_ids, rigid_body_ids, max_trajectories)
        
        print(f"âœ… SE3TrajectoryDataset initialized")
        print(f"   Split: {split}")
        print(f"   Trajectories: {len(self.trajectory_list)}")
        print(f"   Environments: {len(set([t['env_id'] for t in self.trajectory_list]))}")
        print(f"   Augmentation: {self.augmentation}")
    
    def _collect_trajectory_data(self, 
                               env_ids: Optional[List[str]], 
                               rigid_body_ids: Optional[List[int]],
                               max_trajectories: Optional[int]):
        """ê¶¤ì  ë°ì´í„° ìˆ˜ì§‘"""
        self.trajectory_list = []
        
        # í™˜ê²½ ëª©ë¡ ê²°ì •
        available_envs = self.hdf5_loader.list_environments()
        if env_ids is None:
            env_ids = available_envs
        else:
            env_ids = [env for env in env_ids if env in available_envs]
        
        # ë¡œë´‡ ëª©ë¡ ê²°ì •
        available_rbs = self.hdf5_loader.list_rigid_bodies()
        if rigid_body_ids is None:
            rigid_body_ids = available_rbs
        else:
            rigid_body_ids = [rb for rb in rigid_body_ids if rb in available_rbs]
        
        print(f"ğŸ“Š Collecting trajectories from:")
        print(f"   Environments: {len(env_ids)}")
        print(f"   Rigid bodies: {rigid_body_ids}")
        
        # ë°ì´í„° ë¶„í•  (fm-main íŒ¨í„´ ì°¸ì¡°)
        total_collected = 0
        
        for env_id in env_ids:
            try:
                # í™˜ê²½ë³„ ëª¨ë“  ê¶¤ì  ë¡œë“œ
                env_trajectories = self.hdf5_loader.load_trajectories_by_environment(
                    env_id, rb_id=None, trajectory_type=self.trajectory_type, output_format='7d'
                )
                
                if not env_trajectories:
                    continue
                
                # ê¶¤ì  ì¸ë±ìŠ¤ ì •ë ¬
                pair_indices = sorted(env_trajectories.keys())
                
                # ë°ì´í„° ë¶„í•  (fm-main ìŠ¤íƒ€ì¼)
                num_trajectories = len(pair_indices)
                num_val_data = num_test_data = num_trajectories // 5
                num_train_data = num_trajectories - num_val_data - num_test_data
                
                if self.split == 'train':
                    split_indices = pair_indices[:num_train_data]
                elif self.split == 'valid':
                    split_indices = pair_indices[num_train_data:num_train_data+num_val_data]
                elif self.split == 'test':
                    split_indices = pair_indices[num_train_data+num_val_data:]
                else:
                    raise ValueError(f"Unknown split: {self.split}")
                
                # ê° ë¡œë´‡ì— ëŒ€í•´ ê¶¤ì  ìˆ˜ì§‘
                for rb_id in rigid_body_ids:
                    for pair_index in split_indices:
                        traj_data = self.hdf5_loader.load_trajectory(
                            env_id, rb_id, pair_index, self.trajectory_type, '7d'
                        )
                        
                        if traj_data is not None and len(traj_data) > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                            trajectory_info = {
                                'env_id': env_id,
                                'rb_id': rb_id,
                                'pair_index': pair_index,
                                'raw_trajectory': traj_data  # [N, 7] numpy array
                            }
                            self.trajectory_list.append(trajectory_info)
                            total_collected += 1
                            
                            # ìµœëŒ€ ê¶¤ì  ìˆ˜ ì œí•œ
                            if max_trajectories and total_collected >= max_trajectories:
                                break
                    
                    if max_trajectories and total_collected >= max_trajectories:
                        break
                
                if max_trajectories and total_collected >= max_trajectories:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ Error loading environment {env_id}: {e}")
                continue
        
        print(f"ğŸ“‹ Collected {len(self.trajectory_list)} trajectories for {self.split}")
        
        if len(self.trajectory_list) == 0:
            raise RuntimeError("No valid trajectories found")
    
    def _process_trajectory(self, raw_traj_7d: np.ndarray) -> Dict[str, torch.Tensor]:
        """
        SE(3) ê¶¤ì  ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        7D quaternion â†’ SE(3) matrix â†’ ìŠ¤ë¬´ë”© â†’ ë¦¬ìƒ˜í”Œë§ â†’ ë¼ë²¨ ìƒì„±
        """
        # 1) 7D quaternion â†’ SE(3) matrices
        N = raw_traj_7d.shape[0]
        T_raw = torch.zeros(N, 4, 4, dtype=torch.float32)
        
        for i in range(N):
            pose_7d = raw_traj_7d[i]  # [x, y, z, qw, qx, qy, qz]
            
            # Position
            T_raw[i, :3, 3] = torch.tensor(pose_7d[:3])
            
            # Quaternion â†’ Rotation matrix
            qw, qx, qy, qz = pose_7d[3], pose_7d[4], pose_7d[5], pose_7d[6]
            
            # Quaternion to rotation matrix conversion
            R = torch.zeros(3, 3)
            R[0, 0] = 1 - 2*(qy*qy + qz*qz)
            R[0, 1] = 2*(qx*qy - qw*qz)
            R[0, 2] = 2*(qx*qz + qw*qy)
            R[1, 0] = 2*(qx*qy + qw*qz)
            R[1, 1] = 1 - 2*(qx*qx + qz*qz)
            R[1, 2] = 2*(qy*qz - qw*qx)
            R[2, 0] = 2*(qx*qz - qw*qy)
            R[2, 1] = 2*(qy*qz + qw*qx)
            R[2, 2] = 1 - 2*(qx*qx + qy*qy)
            
            T_raw[i, :3, :3] = R
            T_raw[i, 3, 3] = 1.0
        
        # 2) SE(3) ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©
        T_processed, dt_seq, xi_labels, T_smooth = traj_process_se3_pipeline(
            T_raw,
            smooth_first=self.use_smoothing,
            smooth=self.smooth_strength,
            num_samples=self.num_samples,
            lambda_rot=self.lambda_rot,
            policy=self.time_policy,
            v_ref=self.v_ref,
            v_cap=self.v_cap,
            a_lat_max=self.a_lat_max
        )
        
        return {
            'T_raw': T_raw,                    # [N, 4, 4]
            'T_processed': T_processed,        # [M, 4, 4] 
            'dt_seq': dt_seq,                  # [M-1]
            'xi_labels': xi_labels,            # [M, 6]
            'T_smooth': T_smooth if T_smooth is not None else T_raw  # [N, 4, 4]
        }
    
    def _apply_augmentation(self, T: torch.Tensor) -> torch.Tensor:
        """ë°ì´í„° ì¦ê°• ì ìš© (fm-main íŒ¨í„´ ì°¸ì¡°)"""
        if not self.augmentation:
            return T
        
        T_aug = T.clone()
        
        # 1) Random rotation around Z-axis (fm-main ìŠ¤íƒ€ì¼)
        if torch.rand(1) > 0.5:
            theta = torch.rand(1) * 2 * np.pi
            cos_theta, sin_theta = torch.cos(theta), torch.sin(theta)
            
            R_z = torch.eye(4)
            R_z[0, 0] = cos_theta
            R_z[0, 1] = -sin_theta
            R_z[1, 0] = sin_theta
            R_z[1, 1] = cos_theta
            
            # Apply rotation to all poses
            T_aug = R_z @ T_aug
        
        # 2) Add noise to positions
        if self.position_noise_std > 0:
            pos_noise = torch.randn_like(T_aug[:, :3, 3]) * self.position_noise_std
            T_aug[:, :3, 3] += pos_noise
        
        # 3) Add rotation noise (small random rotations)
        if self.rotation_noise_std > 0:
            for i in range(T_aug.shape[0]):
                # Small random rotation around random axis
                axis = torch.randn(3)
                axis = axis / torch.norm(axis)
                angle = torch.randn(1) * self.rotation_noise_std
                
                # Rodrigues formula for rotation
                K = torch.zeros(3, 3)
                K[0, 1] = -axis[2]
                K[0, 2] = axis[1]
                K[1, 0] = axis[2]
                K[1, 2] = -axis[0]
                K[2, 0] = -axis[1]
                K[2, 1] = axis[0]
                
                R_noise = torch.eye(3) + torch.sin(angle) * K + (1 - torch.cos(angle)) * (K @ K)
                T_aug[i, :3, :3] = T_aug[i, :3, :3] @ R_noise
        
        return T_aug
    
    def __len__(self) -> int:
        """ë°ì´í„°ì…‹ í¬ê¸°"""
        return len(self.trajectory_list)
    
    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:
        """
        ë‹¨ì¼ ê¶¤ì  ë°ì´í„° ë°˜í™˜
        
        Returns:
            Dict containing:
            - 'T_processed': [M, 4, 4] ì²˜ë¦¬ëœ SE(3) ê¶¤ì 
            - 'xi_labels': [M, 6] body twist ë¼ë²¨
            - 'dt_seq': [M-1] ì‹œê°„ ê°„ê²©
            - 'T_raw': [N, 4, 4] ì›ì‹œ ê¶¤ì  (ì°¸ì¡°ìš©)
            - 'T_smooth': [N, 4, 4] ìŠ¤ë¬´ë”©ëœ ê¶¤ì 
            - 'metadata': ë©”íƒ€ë°ì´í„°
        """
        traj_info = self.trajectory_list[index]
        
        # ê¶¤ì  ì²˜ë¦¬
        processed_data = self._process_trajectory(traj_info['raw_trajectory'])
        
        # ì¦ê°• ì ìš© (í›ˆë ¨ ì‹œì—ë§Œ)
        T_processed = self._apply_augmentation(processed_data['T_processed'])
        T_raw = self._apply_augmentation(processed_data['T_raw'])
        T_smooth = self._apply_augmentation(processed_data['T_smooth'])
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            'env_id': traj_info['env_id'],
            'rb_id': traj_info['rb_id'], 
            'pair_index': traj_info['pair_index'],
            'split': self.split,
            'original_length': len(traj_info['raw_trajectory']),
            'processed_length': len(T_processed)
        }
        
        return {
            'T_processed': T_processed,           # [M, 4, 4]
            'xi_labels': processed_data['xi_labels'],  # [M, 6]
            'dt_seq': processed_data['dt_seq'],        # [M-1]
            'T_raw': T_raw,                       # [N, 4, 4]
            'T_smooth': T_smooth,                 # [N, 4, 4]
            'metadata': metadata
        }
    
    def get_dataloader(self, batch_size: int = 32, shuffle: Optional[bool] = None, 
                      num_workers: int = 4, **kwargs) -> torch.utils.data.DataLoader:
        """
        DataLoader ìƒì„± (fm-main íŒ¨í„´ ì°¸ì¡°)
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
            shuffle: ì…”í”Œ ì—¬ë¶€ (Noneì´ë©´ splitì— ë”°ë¼ ìë™ ê²°ì •)
            num_workers: worker ìˆ˜
            **kwargs: DataLoader ì¶”ê°€ ì¸ìˆ˜
        
        Returns:
            torch.utils.data.DataLoader
        """
        if shuffle is None:
            shuffle = (self.split == 'train')
        
        return torch.utils.data.DataLoader(
            self,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self._collate_fn,
            **kwargs
        )
    
    def _collate_fn(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        ë°°ì¹˜ ìƒì„±ì„ ìœ„í•œ collate function
        ê° ê¶¤ì ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒ¨ë”© ì²˜ë¦¬
        """
        # ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°
        max_len_processed = max(len(item['T_processed']) for item in batch)
        max_len_raw = max(len(item['T_raw']) for item in batch)
        batch_size = len(batch)
        
        # íŒ¨ë”©ëœ í…ì„œ ìƒì„±
        T_processed_batch = torch.zeros(batch_size, max_len_processed, 4, 4)
        xi_labels_batch = torch.zeros(batch_size, max_len_processed, 6)
        dt_seq_batch = torch.zeros(batch_size, max_len_processed - 1)
        T_raw_batch = torch.zeros(batch_size, max_len_raw, 4, 4)
        T_smooth_batch = torch.zeros(batch_size, max_len_raw, 4, 4)
        
        # ê¸¸ì´ ë§ˆìŠ¤í¬
        lengths_processed = torch.zeros(batch_size, dtype=torch.long)
        lengths_raw = torch.zeros(batch_size, dtype=torch.long)
        
        # ë©”íƒ€ë°ì´í„°
        metadata_batch = []
        
        for i, item in enumerate(batch):
            len_processed = len(item['T_processed'])
            len_raw = len(item['T_raw'])
            
            T_processed_batch[i, :len_processed] = item['T_processed']
            xi_labels_batch[i, :len_processed] = item['xi_labels']
            dt_seq_batch[i, :len_processed-1] = item['dt_seq']
            T_raw_batch[i, :len_raw] = item['T_raw']
            T_smooth_batch[i, :len_raw] = item['T_smooth']
            
            lengths_processed[i] = len_processed
            lengths_raw[i] = len_raw
            metadata_batch.append(item['metadata'])
        
        return {
            'T_processed': T_processed_batch,    # [B, M, 4, 4]
            'xi_labels': xi_labels_batch,        # [B, M, 6]
            'dt_seq': dt_seq_batch,              # [B, M-1]
            'T_raw': T_raw_batch,                # [B, N, 4, 4]
            'T_smooth': T_smooth_batch,          # [B, N, 4, 4]
            'lengths_processed': lengths_processed,  # [B]
            'lengths_raw': lengths_raw,          # [B]
            'metadata': metadata_batch           # List[Dict]
        }
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'hdf5_loader'):
            self.hdf5_loader.close()
    
    def __del__(self):
        """ì†Œë©¸ì"""
        self.close()


def get_se3_dataloaders(hdf5_path: str, 
                       train_config: Dict[str, Any],
                       val_config: Dict[str, Any],
                       test_config: Dict[str, Any]) -> Tuple[torch.utils.data.DataLoader, 
                                                           torch.utils.data.DataLoader,
                                                           torch.utils.data.DataLoader]:
    """
    SE(3) ë°ì´í„°ë¡œë” íŒ©í† ë¦¬ í•¨ìˆ˜ (fm-main ìŠ¤íƒ€ì¼)
    
    Args:
        hdf5_path: HDF5 íŒŒì¼ ê²½ë¡œ
        train_config: í›ˆë ¨ ì„¤ì •
        val_config: ê²€ì¦ ì„¤ì •  
        test_config: í…ŒìŠ¤íŠ¸ ì„¤ì •
    
    Returns:
        (train_loader, val_loader, test_loader)
    """
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SE3TrajectoryDataset(hdf5_path, split='train', **train_config)
    val_dataset = SE3TrajectoryDataset(hdf5_path, split='valid', **val_config)
    test_dataset = SE3TrajectoryDataset(hdf5_path, split='test', **test_config)
    
    # DataLoader ìƒì„±
    train_loader = train_dataset.get_dataloader(shuffle=True)
    val_loader = val_dataset.get_dataloader(shuffle=False)
    test_loader = test_dataset.get_dataloader(shuffle=False)
    
    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª Testing SE3TrajectoryDataset")
    
    test_hdf5_path = "/Users/a123/Documents/Projects/2D_sim/packages/data_generator/test_trajectory_dataset.h5"
    
    try:
        # ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
        config = {
            'use_smoothing': True,
            'smooth_strength': 0.1,
            'num_samples': 50,
            'augmentation': True,
            'max_trajectories': 5
        }
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = SE3TrajectoryDataset(test_hdf5_path, split='train', **config)
        
        print(f"\nğŸ“Š Dataset Info:")
        print(f"   Length: {len(dataset)}")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        if len(dataset) > 0:
            sample = dataset[0]
            print(f"\nğŸ” Sample Data:")
            for key, value in sample.items():
                if isinstance(value, torch.Tensor):
                    print(f"   {key}: {value.shape}")
                else:
                    print(f"   {key}: {value}")
            
            # DataLoader í…ŒìŠ¤íŠ¸
            dataloader = dataset.get_dataloader(batch_size=2)
            batch = next(iter(dataloader))
            print(f"\nğŸ“¦ Batch Data:")
            for key, value in batch.items():
                if isinstance(value, torch.Tensor):
                    print(f"   {key}: {value.shape}")
                else:
                    print(f"   {key}: {len(value) if isinstance(value, list) else value}")
        
        dataset.close()
        print("\nâœ… Test completed successfully!")
        
    except FileNotFoundError:
        print(f"âš ï¸ Test file not found: {test_hdf5_path}")
        print("   Create HDF5 data first using hdf5_schema_creator.py")
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()