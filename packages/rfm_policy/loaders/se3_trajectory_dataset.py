"""
SE(3) Trajectory Dataset for RFM Training

This module provides data loading and preprocessing for SE(3) trajectory data
generated by the B-spline smoothing pipeline. Supports flow matching training
with point cloud environments.
"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
from typing import Dict, List, Tuple, Optional, Any
import glob
from pathlib import Path

from ..utils.se3_utils import SE3Utils


class SE3TrajectoryDataset(Dataset):
    """
    Dataset for SE(3) trajectory data with point cloud environments
    
    Loads trajectory JSON files and corresponding point cloud data
    for training SE(3) Riemannian Flow Matching models.
    """
    
    def __init__(
        self,
        data_root: str,
        trajectory_pattern: str = "*.json",
        pointcloud_root: str = None,
        max_trajectories: Optional[int] = None,
        max_points_per_cloud: int = 1024,
        augment_data: bool = True,
        use_bsplined: bool = True,
        subsample_factor: int = 1,
        cache_pointclouds: bool = True
    ):
        """
        Initialize SE(3) trajectory dataset
        
        Args:
            data_root: Root directory containing trajectory files
            trajectory_pattern: Pattern to match trajectory files
            pointcloud_root: Root directory for point cloud files (inferred if None)
            max_trajectories: Maximum number of trajectories to load
            max_points_per_cloud: Maximum points per point cloud
            augment_data: Whether to apply data augmentation
            use_bsplined: Whether to use B-splined trajectories
            subsample_factor: Factor to subsample trajectory waypoints
            cache_pointclouds: Whether to cache point clouds in memory
        """
        self.data_root = Path(data_root)
        self.trajectory_pattern = trajectory_pattern
        self.max_points_per_cloud = max_points_per_cloud
        self.augment_data = augment_data
        self.use_bsplined = use_bsplined
        self.subsample_factor = subsample_factor
        self.cache_pointclouds = cache_pointclouds
        
        # SE(3) utilities
        self.se3_utils = SE3Utils()
        
        # Point cloud cache
        self.pointcloud_cache = {}
        
        # Set point cloud root
        if pointcloud_root is None:
            # Infer from data structure
            self.pointcloud_root = self.data_root.parent / "pointcloud"
        else:
            self.pointcloud_root = Path(pointcloud_root)
        
        # Load trajectory file paths
        self.trajectory_files = self._find_trajectory_files(max_trajectories)
        
        # Build trajectory segments for training
        self.trajectory_segments = self._build_trajectory_segments()
        
        print(f"Loaded {len(self.trajectory_files)} trajectories")
        print(f"Generated {len(self.trajectory_segments)} training segments")
    
    def _find_trajectory_files(self, max_trajectories: Optional[int]) -> List[Path]:
        """Find trajectory files matching the pattern"""
        pattern = self.trajectory_pattern
        if self.use_bsplined:
            # Prefer B-splined trajectories
            pattern = pattern.replace('.json', '_bsplined.json')
        
        trajectory_files = list(self.data_root.glob(pattern))
        
        if max_trajectories is not None:
            trajectory_files = trajectory_files[:max_trajectories]
        
        return sorted(trajectory_files)
    
    def _build_trajectory_segments(self) -> List[Dict[str, Any]]:
        """
        Build trajectory segments for training
        
        Each segment contains:
        - trajectory file path
        - segment start/end indices  
        - environment info
        - robot geometry
        """
        segments = []
        
        for traj_file in self.trajectory_files:
            try:
                with open(traj_file, 'r') as f:
                    traj_data = json.load(f)
                
                # Extract trajectory path
                path_data = traj_data['path']['data']
                n_waypoints = len(path_data)
                
                # Subsample if needed
                if self.subsample_factor > 1:
                    indices = list(range(0, n_waypoints, self.subsample_factor))
                    if indices[-1] != n_waypoints - 1:
                        indices.append(n_waypoints - 1)  # Always include end
                else:
                    indices = list(range(n_waypoints))
                
                # Create segments (could use sliding window for data augmentation)
                if len(indices) >= 2:
                    segment = {
                        'trajectory_file': traj_file,
                        'waypoint_indices': indices,
                        'environment_name': traj_data['environment']['name'],
                        'robot_geometry': traj_data['rigid_body'],
                        'metadata': traj_data.get('bspline_metadata', {})
                    }
                    segments.append(segment)
                    
            except Exception as e:
                print(f"Warning: Failed to load {traj_file}: {e}")
                continue
        
        return segments
    
    def _load_point_cloud(self, environment_name: str) -> torch.Tensor:
        """Load point cloud for environment"""
        if environment_name in self.pointcloud_cache:
            return self.pointcloud_cache[environment_name]
        
        # Find point cloud file
        ply_file = self.pointcloud_root / environment_name / f"{environment_name}.ply"
        
        if not ply_file.exists():
            # Try alternative paths
            alt_paths = [
                self.pointcloud_root / "circles_only" / "circles_only.ply",
                self.data_root.parent / "pointcloud" / "circles_only" / "circles_only.ply"
            ]
            
            for alt_path in alt_paths:
                if alt_path.exists():
                    ply_file = alt_path
                    break
            else:
                raise FileNotFoundError(f"Point cloud file not found for {environment_name}")
        
        # Load PLY file
        points = self._load_ply_file(ply_file)
        
        # Subsample points if needed
        if len(points) > self.max_points_per_cloud:
            indices = np.random.choice(len(points), self.max_points_per_cloud, replace=False)
            points = points[indices]
        
        points_tensor = torch.from_numpy(points).float()
        
        # Cache if enabled
        if self.cache_pointclouds:
            self.pointcloud_cache[environment_name] = points_tensor
        
        return points_tensor
    
    def _load_ply_file(self, ply_file: Path) -> np.ndarray:
        """Load PLY file and extract 3D points"""
        try:
            # Simple PLY loader (assumes ASCII format)
            points = []
            
            with open(ply_file, 'r') as f:
                lines = f.readlines()
            
            # Find header end
            header_end = 0
            num_vertices = 0
            
            for i, line in enumerate(lines):
                if line.startswith('element vertex'):
                    num_vertices = int(line.split()[-1])
                elif line.startswith('end_header'):
                    header_end = i + 1
                    break
            
            # Read vertex data
            for i in range(header_end, header_end + num_vertices):
                parts = lines[i].strip().split()
                if len(parts) >= 3:
                    x, y = float(parts[0]), float(parts[1])
                    z = 0.0  # Assume 2D environment, set z=0
                    points.append([x, y, z])
            
            return np.array(points, dtype=np.float32)
            
        except Exception as e:
            print(f"Warning: Failed to load PLY file {ply_file}: {e}")
            # Return dummy points
            return np.random.randn(100, 3).astype(np.float32)
    
    def _convert_se3_trajectory(self, path_data: List[List[float]]) -> torch.Tensor:
        """Convert trajectory data to SE(3) matrices"""
        trajectory = []
        
        for pose_data in path_data:
            # pose_data format: [x, y, z, rx, ry, rz] (SE(3) format)
            x, y, z = pose_data[0], pose_data[1], pose_data[2]
            rx, ry, rz = pose_data[3], pose_data[4], pose_data[5]
            
            # Create SE(3) matrix
            T = torch.eye(4)
            T[0, 3] = x
            T[1, 3] = y  
            T[2, 3] = z
            
            # Convert rotation (assuming rz is yaw for 2D case, rx=ry=0)
            cos_rz = torch.cos(torch.tensor(rz))
            sin_rz = torch.sin(torch.tensor(rz))
            
            T[0, 0] = cos_rz
            T[0, 1] = -sin_rz
            T[1, 0] = sin_rz
            T[1, 1] = cos_rz
            # T[2, 2] = 1 (already set by eye)
            
            trajectory.append(T)
        
        return torch.stack(trajectory)  # [n_waypoints, 4, 4]
    
    def _augment_data(
        self, 
        trajectory: torch.Tensor, 
        point_cloud: torch.Tensor,
        geometry: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Apply data augmentation"""
        if not self.augment_data:
            return trajectory, point_cloud, geometry
        
        # Random SE(3) transformation for augmentation
        batch_size = 1
        aug_transform = self.se3_utils.sample_gaussian_se3(
            batch_size, std=0.1, device=trajectory.device
        )[0]  # [4, 4]
        
        # Apply to trajectory
        aug_trajectory = torch.stack([
            self.se3_utils.compose_se3(aug_transform, pose) 
            for pose in trajectory
        ])
        
        # Apply to point cloud (extract rotation and translation)
        R_aug = aug_transform[:3, :3]  # [3, 3]
        t_aug = aug_transform[:3, 3]   # [3]
        
        aug_point_cloud = torch.matmul(point_cloud, R_aug.T) + t_aug.unsqueeze(0)
        
        # Geometry stays the same (intrinsic property)
        aug_geometry = geometry
        
        return aug_trajectory, aug_point_cloud, aug_geometry
    
    def __len__(self) -> int:
        return len(self.trajectory_segments)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get training sample
        
        Returns:
            sample: Dictionary containing:
                - trajectory: [n_waypoints, 4, 4] SE(3) trajectory
                - point_cloud: [n_points, 3] point cloud
                - geometry: [3] ellipsoid parameters (a, b, c)
                - start_pose: [4, 4] start pose
                - goal_pose: [4, 4] goal pose
                - metadata: dict with additional info
        """
        segment = self.trajectory_segments[idx]
        
        # Load trajectory data
        with open(segment['trajectory_file'], 'r') as f:
            traj_data = json.load(f)
        
        # Extract relevant waypoints
        path_data = traj_data['path']['data']
        waypoint_indices = segment['waypoint_indices']
        selected_waypoints = [path_data[i] for i in waypoint_indices]
        
        # Convert to SE(3) trajectory
        trajectory = self._convert_se3_trajectory(selected_waypoints)
        
        # Load point cloud
        point_cloud = self._load_point_cloud(segment['environment_name'])
        
        # Extract geometry
        robot_geom = segment['robot_geometry']
        geometry = torch.tensor([
            robot_geom['semi_major_axis'],
            robot_geom['semi_minor_axis'],
            robot_geom.get('semi_minor_axis', robot_geom['semi_minor_axis'])  # Assume ellipse -> ellipsoid
        ], dtype=torch.float32)
        
        # Apply augmentation
        trajectory, point_cloud, geometry = self._augment_data(trajectory, point_cloud, geometry)
        
        # Extract start and goal poses
        start_pose = trajectory[0]   # [4, 4]
        goal_pose = trajectory[-1]   # [4, 4]
        
        return {
            'trajectory': trajectory,           # [n_waypoints, 4, 4]
            'point_cloud': point_cloud,         # [n_points, 3]
            'geometry': geometry,               # [3]
            'start_pose': start_pose,           # [4, 4]
            'goal_pose': goal_pose,             # [4, 4]
            'trajectory_file': str(segment['trajectory_file']),
            'environment_name': segment['environment_name'],
            'metadata': segment['metadata']
        }


def collate_se3_trajectories(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    """
    Custom collate function for SE(3) trajectory data
    
    Handles variable-length trajectories and point clouds.
    """
    # Separate data by type
    trajectories = [item['trajectory'] for item in batch]
    point_clouds = [item['point_cloud'] for item in batch]
    geometries = [item['geometry'] for item in batch]
    start_poses = [item['start_pose'] for item in batch]
    goal_poses = [item['goal_pose'] for item in batch]
    
    # Stack geometries, start/goal poses (fixed size)
    batch_geometry = torch.stack(geometries)      # [B, 3]
    batch_start_poses = torch.stack(start_poses)  # [B, 4, 4]
    batch_goal_poses = torch.stack(goal_poses)    # [B, 4, 4]
    
    # Handle variable-length trajectories
    max_traj_len = max(traj.shape[0] for traj in trajectories)
    batch_size = len(trajectories)
    
    batch_trajectories = torch.zeros(batch_size, max_traj_len, 4, 4)
    trajectory_lengths = torch.zeros(batch_size, dtype=torch.long)
    
    for i, traj in enumerate(trajectories):
        traj_len = traj.shape[0]
        batch_trajectories[i, :traj_len] = traj
        trajectory_lengths[i] = traj_len
    
    # Handle variable-length point clouds
    max_pc_len = max(pc.shape[0] for pc in point_clouds)
    batch_point_clouds = torch.zeros(batch_size, max_pc_len, 3)
    point_cloud_lengths = torch.zeros(batch_size, dtype=torch.long)
    
    for i, pc in enumerate(point_clouds):
        pc_len = pc.shape[0]
        batch_point_clouds[i, :pc_len] = pc
        point_cloud_lengths[i] = pc_len
    
    # Collect metadata
    trajectory_files = [item['trajectory_file'] for item in batch]
    environment_names = [item['environment_name'] for item in batch]
    
    return {
        'trajectories': batch_trajectories,           # [B, max_traj_len, 4, 4]
        'trajectory_lengths': trajectory_lengths,     # [B]
        'point_clouds': batch_point_clouds,           # [B, max_pc_len, 3]
        'point_cloud_lengths': point_cloud_lengths,   # [B]
        'geometries': batch_geometry,                 # [B, 3]
        'start_poses': batch_start_poses,             # [B, 4, 4]
        'goal_poses': batch_goal_poses,               # [B, 4, 4]
        'trajectory_files': trajectory_files,
        'environment_names': environment_names
    }


def create_se3_dataloader(
    data_root: str,
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 4,
    **dataset_kwargs
) -> DataLoader:
    """
    Create DataLoader for SE(3) trajectory data
    
    Args:
        data_root: Root directory containing trajectory files
        batch_size: Batch size
        shuffle: Whether to shuffle data
        num_workers: Number of worker processes
        **dataset_kwargs: Additional arguments for SE3TrajectoryDataset
        
    Returns:
        dataloader: DataLoader instance
    """
    dataset = SE3TrajectoryDataset(data_root, **dataset_kwargs)
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_se3_trajectories,
        pin_memory=True
    )


# =============================================================================
# Utility Functions
# =============================================================================

def print_dataset_statistics(dataset: SE3TrajectoryDataset):
    """Print statistics about the dataset"""
    print(f"\n=== SE(3) Trajectory Dataset Statistics ===")
    print(f"Number of trajectories: {len(dataset.trajectory_files)}")
    print(f"Number of training segments: {len(dataset.trajectory_segments)}")
    
    # Sample a few items to get statistics
    sample_size = min(10, len(dataset))
    traj_lengths = []
    pc_lengths = []
    
    for i in range(sample_size):
        sample = dataset[i]
        traj_lengths.append(sample['trajectory'].shape[0])
        pc_lengths.append(sample['point_cloud'].shape[0])
    
    print(f"Average trajectory length: {np.mean(traj_lengths):.1f} ± {np.std(traj_lengths):.1f}")
    print(f"Average point cloud size: {np.mean(pc_lengths):.1f} ± {np.std(pc_lengths):.1f}")
    print(f"Data augmentation: {'Enabled' if dataset.augment_data else 'Disabled'}")
    print(f"Using B-splined trajectories: {'Yes' if dataset.use_bsplined else 'No'}")


def visualize_sample(sample: Dict[str, torch.Tensor], save_path: str = None):
    """Visualize a dataset sample"""
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    
    fig = plt.figure(figsize=(15, 5))
    
    # Extract data
    trajectory = sample['trajectory'].numpy()  # [n_waypoints, 4, 4]
    point_cloud = sample['point_cloud'].numpy()  # [n_points, 3]
    geometry = sample['geometry'].numpy()  # [3]
    
    # Extract trajectory positions
    positions = trajectory[:, :3, 3]  # [n_waypoints, 3]
    
    # Plot 1: 3D trajectory and point cloud
    ax1 = fig.add_subplot(131, projection='3d')
    ax1.scatter(point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2], 
               c='red', s=1, alpha=0.6, label='Obstacles')
    ax1.plot(positions[:, 0], positions[:, 1], positions[:, 2], 
             'b-o', linewidth=2, markersize=4, label='Trajectory')
    ax1.scatter(positions[0, 0], positions[0, 1], positions[0, 2], 
               c='green', s=100, label='Start')
    ax1.scatter(positions[-1, 0], positions[-1, 1], positions[-1, 2], 
               c='red', s=100, label='Goal')
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax1.set_zlabel('Z')
    ax1.legend()
    ax1.set_title('3D Trajectory')
    
    # Plot 2: 2D top-down view
    ax2 = fig.add_subplot(132)
    ax2.scatter(point_cloud[:, 0], point_cloud[:, 1], 
               c='red', s=1, alpha=0.6, label='Obstacles')
    ax2.plot(positions[:, 0], positions[:, 1], 
             'b-o', linewidth=2, markersize=4, label='Trajectory')
    ax2.scatter(positions[0, 0], positions[0, 1], 
               c='green', s=100, label='Start')
    ax2.scatter(positions[-1, 0], positions[-1, 1], 
               c='red', s=100, label='Goal')
    ax2.set_xlabel('X')
    ax2.set_ylabel('Y')
    ax2.legend()
    ax2.set_title('Top-down View')
    ax2.axis('equal')
    
    # Plot 3: Robot geometry visualization
    ax3 = fig.add_subplot(133)
    a, b, c = geometry
    theta = np.linspace(0, 2*np.pi, 100)
    ellipse_x = a * np.cos(theta)
    ellipse_y = b * np.sin(theta)
    ax3.plot(ellipse_x, ellipse_y, 'g-', linewidth=2, label=f'Robot (a={a:.2f}, b={b:.2f})')
    ax3.set_xlabel('X')
    ax3.set_ylabel('Y')
    ax3.legend()
    ax3.set_title('Robot Geometry')
    ax3.axis('equal')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Visualization saved to {save_path}")
    
    plt.show()